# -*- coding: utf-8 -*-
"""CS267-ReviewerPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VnvewjsDmquNNtObrSijo70X4NwNW0VM
"""


output_string=""
import nltk
from nltk.corpus import stopwords

#nltk.download('stopwords')
#nltk.download('punkt')
#print(stopwords.words('english'))

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
def preprocessing(sentence):
  remove_chars = [';', ':', '!', "*", "-","(",")","@","%","&","\\",'`','~','#','%','_','+','=']
  #preprocessing 1- remove leading and trailing whitespaces
  sentence=sentence.strip()

  #preprocessing 2 - convert to lowercase
  sentence=sentence.lower()

  #preprocessing 3 - remove special characters
  final_string=""
  for ch in sentence:
    if ch not in remove_chars:
      final_string+=ch
  sentence=final_string

  #preprocessing 4 - remove stop words
  stop_words = set(stopwords.words('english'))
  stop_words.discard('not')
  word_tokens = word_tokenize(sentence)
  filtered_sentence = [w for w in word_tokens if not w in stop_words]
  sentence=filtered_sentence





  return ' '.join(sentence)

# Opening JSON file
f = open('Home_and_Kitchen_5.json', 'r',encoding='iso-8859-1')

data={} #key- reviewer id, value- overall, vote, review

i=0 #used for skipping the header line
skipped=0


#Here we are grouping based on the reviewer id
for lines in f:
  if i!=0:
    #print(i)
    try:
      info=lines.split(", \"")
      if "image" in info[0]:
        info=info[1:]
      if "overall" in info[0]:

        overall=info[0].split(": ")[1]
      else:
        overall=0
      if "vote" in info[1]:
        vote=info[1].split(": \"")[1]
        vote=vote[:-1]
        id=info[4].split(": \"")[1][:-1]
        if "style" in info[6]:
          review=info[8].split(": \"")
        else:
          review=info[7].split(": \"")

      else:
        vote=0
        id=info[3].split(": \"")[1]
        if "style" in info[5]:
          review=info[7].split(": \"")
        else:
          review=info[6].split(": \"")
      if len(review)==2:
          review=review[1][:-1]
      else:
          review=""
      review=preprocessing(review)
      to_add=[float(overall),float(vote),review]
      id=id.replace("\"","")
      #reducer code
      if id not in data:
        data[id]=[]
      data[id].append(to_add)
    except:
      #print("skipped row",i)
      skipped+=1
  
  i+=1


print("Total reviewers",len(data))
#for row in data:
  #print(row,data[row])
print("Skipped rows",skipped)
f.close()




#mapper sends this data, where the pair step is replaced by \t

#quick check to see if the code is working as expected
m=0
for row in data:
  m=max(m,len(data[row]))
print("Max number of reviews for any reviewer",m)

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer

def tfidf_calc(reviews):
  # Create a TfidfVectorizer object
  vectorizer = TfidfVectorizer()

  # Fit the vectorizer to the documents
  vectorizer.fit(reviews)

  # Transform the documents into TF-IDF vectors
  tf_idf_matrix = vectorizer.transform(reviews)

  # Convert the sparse TF-IDF matrix to a dense representation (NumPy array)
  dense_tf_idf_matrix = tf_idf_matrix.toarray()

  #print("TF-IDF matrix:")
  #print(dense_tf_idf_matrix)
  return dense_tf_idf_matrix

def determine_genuine_or_fake(predictions):
  count=0
  total=len(predictions)
  for value in predictions:
    if value<0:
      count+=1
  if count>(0.5*total):
    return True
  else:
    return False

def isolation_forest(combined_data):
  #Data split up into test and training datasets, this is the final block of code with numerical features added
  #print("combined_data",len(combined_data))
  

  from sklearn.ensemble import IsolationForest
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import precision_recall_curve, roc_auc_score,f1_score

  # Split data into training and testing sets
  X_train, X_test = train_test_split(combined_data, test_size=0.2)

  # Train Isolation Forest model
  iso_forest = IsolationForest(n_estimators=100)
  iso_forest.fit(X_train)

  # Predict anomalies on testing data
  predicted_anomalies = iso_forest.predict(X_test)
  #print("Predicted anomalies",predicted_anomalies)
  return predicted_anomalies

def performance_measure(predicted_anomalies):
  import pandas as pd
  import numpy as np
  from sklearn.cluster import KMeans
  from sklearn.metrics import silhouette_score,davies_bouldin_score
  from sklearn.exceptions import ConvergenceWarning
  import warnings
  silhouette_avg=0
  db_score=10000
  try:
    normal_data = [x for x in predicted_anomalies if x>0]
    normal_data.append(5)
    normal_data.append(5)
    normal_data.append(5)
    
    #print("normal data",normal_data)
    normal_data = np.array(normal_data)
    normal_data=normal_data.reshape(-1, 1)
    with warnings.catch_warnings():
      # Ignore DeprecationWarnings
      warnings.filterwarnings(action='ignore', category=ConvergenceWarning)
      # Perform k-means clustering
      kmeans = KMeans(n_clusters=2)
      kmeans.fit(normal_data)  # Reshape data to a 2D array
      labels = kmeans.predict(normal_data)
      kmeans.fit(normal_data)
      silhouette_avg = silhouette_score(normal_data, kmeans.labels_)
      db_score = davies_bouldin_score(normal_data, labels)
    #print("Davies-Bouldin Index:", db_score)
    #print("Silhouette score",silhouette_avg)
  except:
    pass
  return [silhouette_avg,db_score]

def dbscan(combined_data):

  X_train, X_test = train_test_split(combined_data, test_size=0.2)
  # Apply DBSCAN
  dbscan_model = DBSCAN(eps=1, min_samples=2)  # Adjust parameters based on your data
  output = dbscan_model.fit_predict(X_train)
  return output

from sklearn.svm import OneClassSVM
from sklearn.model_selection import train_test_split

def oneClassSVM_Method(combined_data):
  # Split data into training and testing sets
  X_train, X_test = train_test_split(combined_data, test_size=0.1)

  # Train One Class SVM model
  oneSVM = OneClassSVM(gamma='scale')
  oneSVM.fit(X_train)

  # Predict anomalies on testing data
  predicted_anomalies = oneSVM.predict(X_test)

  return predicted_anomalies
  
  
#calculate for every reviewer

#print(("\n\nUsing Isolation Forest"))
overall_silhouette_performance=[]
overall_dbi_performance=[]
fake_iso_list=[]
fake_iso=0
genuine_iso_list=[]
genuine_iso=0
for reviewer in data:
  reviews=[row[2] for row in data[reviewer]]
  if len(reviews)>25:
    matrix=tfidf_calc(reviews)
    #stack all the arrays
    overall_array=[row[0] for row in data[reviewer]]
    vote_array=[row[1] for row in data[reviewer]]
    #print(overall_array)

    # Numerical data
    numerical_data_overall = np.array(overall_array)
    numerical_data_vote = np.array(vote_array)

    # Reshape numerical data to have the same number of elements as text data
    numerical_data_overall = numerical_data_overall.reshape(len(reviews), 1)
    numerical_data_vote = numerical_data_vote.reshape(len(reviews), 1)

    # Concatenate numerical and text data
    combined_data = np.hstack((numerical_data_overall, numerical_data_vote,matrix))
    predictions=isolation_forest(combined_data)
    result=determine_genuine_or_fake(predictions)
    if result:
      #print(reviewer,"is: Fake")
      fake_iso+=1
      fake_iso_list.append(reviewer)
    else:
      #print(reviewer,"is: Genuine")
      genuine_iso+=1
      genuine_iso_list.append(reviewer)
    """performance=performance_measure(predictions)
    overall_silhouette_performance.append(performance[0])
    overall_dbi_performance.append(performance[1])
output_string+="Isolation Forest\n\n"
output_string+="Maximum silhouette score: "+str(max(overall_silhouette_performance))+"\n\n"
output_string+="Best DBI score: "+str(min(overall_dbi_performance))+"\n\n"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import DBSCAN
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split


#calculate for every reviewer, one-class SVM
overall_silhouette_performance=[]
overall_dbi_performance=[]
fake_ocsvm=0
fake_ocsvm_list=[]
genuine_ocsvm=0
genuine_ocsvm_list=[]
#print(("\n\nUsing One-Class SVM"))
for reviewer in data:
  reviews=[row[2] for row in data[reviewer]]
  if len(reviews)>25:
    matrix=tfidf_calc(reviews)
    #stack all the arrays
    overall_array=[row[0] for row in data[reviewer]]
    vote_array=[row[1] for row in data[reviewer]]
    #print(overall_array)

    # Numerical data
    numerical_data_overall = np.array(overall_array)
    numerical_data_vote = np.array(vote_array)

    # Reshape numerical data to have the same number of elements as text data
    numerical_data_overall = numerical_data_overall.reshape(len(reviews), 1)
    numerical_data_vote = numerical_data_vote.reshape(len(reviews), 1)

    # Concatenate numerical and text data
    combined_data = np.hstack((numerical_data_overall, numerical_data_vote,matrix))
    predictions=oneClassSVM_Method(combined_data)
    result=determine_genuine_or_fake(predictions)
    if result:
      #print(reviewer,"is: Fake")
      fake_ocsvm+=1
      fake_ocsvm_list.append(reviewer)

    else:
      #print(reviewer,"is: Genuine")
      genuine_ocsvm+=1
      genuine_ocsvm_list.append(reviewer)
    """performance=performance_measure(predictions)
    overall_silhouette_performance.append(performance[0])
    overall_dbi_performance.append(performance[1])

output_string+="One-Class SVM\n\n"
output_string+="Maximum silhouette score: "+str(max(overall_silhouette_performance))+"\n\n"
output_string+="Best DBI score: "+str(min(overall_dbi_performance))+"\n\n"
"""
#calculate for every reviewer, DBSCAN

overall_silhouette_performance=[]
overall_dbi_performance=[]
fake_dbscan=0
fake_dbscan_list=[]
genuine_dbscan=0
genuine_dbscan_list=[]
#print(("\n\nUsing DBSCAN"))
for reviewer in data:
  reviews=[row[2] for row in data[reviewer]]
  if len(reviews)>25:
    matrix=tfidf_calc(reviews)
    #stack all the arrays
    overall_array=[row[0] for row in data[reviewer]]
    vote_array=[row[1] for row in data[reviewer]]
    #print(overall_array)

    # Numerical data
    numerical_data_overall = np.array(overall_array)
    numerical_data_vote = np.array(vote_array)

    # Reshape numerical data to have the same number of elements as text data
    numerical_data_overall = numerical_data_overall.reshape(len(reviews), 1)
    numerical_data_vote = numerical_data_vote.reshape(len(reviews), 1)

    # Concatenate numerical and text data
    combined_data = np.hstack((numerical_data_overall, numerical_data_vote,matrix))
    predictions=dbscan(combined_data)
    result=determine_genuine_or_fake(predictions)
    if result:
      #print(reviewer,"is: Fake")
      fake_dbscan+=1
      fake_dbscan_list.append(reviewer)
    else:
      #print(reviewer,"is: Genuine")
      genuine_dbscan+=1
      genuine_dbscan_list.append(reviewer)
    """performance=performance_measure(predictions)
    overall_silhouette_performance.append(performance[0])
    overall_dbi_performance.append(performance[1])

output_string+="DBSCAN\n\n"
output_string+="Maximum silhouette score: "+str(max(overall_silhouette_performance))+"\n\n"
output_string+="Best DBI score: "+str(min(overall_dbi_performance))+"\n\n"

print("\n\n\n\n\n\n",output_string)"""

print("Number of fake reviewers")
print("Using Isolation forest",fake_iso)
print("Using DBSCAN",fake_dbscan)
print("Using One-Class SVM",fake_ocsvm)
print("\n\n\n")
print("Number of genuine reviewers")
print("Using Isolation forest",genuine_iso)
print("Using DBSCAN",genuine_dbscan)
print("Using One-Class SVM",genuine_ocsvm)

